{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adc3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Generator, Optional\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "from jaxlobster.constants import MESSAGE_TOKEN_DTYPE_MAP, MESSAGE_TOKEN_TYPES, TIME_COL, get_orderbook_token_types\n",
    "\n",
    "\n",
    "def _batch(iterable: list[str], n=1) -> Generator[list[str], None, None]:\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def _df_to_str(df: pd.DataFrame, n_msgs: int = 1) -> list[str]:\n",
    "    columns = df.columns.tolist()\n",
    "    values = df.to_numpy()\n",
    "    row_strings = [','.join([f\"{col},{val}\" for col, val in zip(columns, row) if not pd.isna(val)]) for row in values]\n",
    "    concatted_strings = ['\\n'.join(rows) for rows in _batch(row_strings, n_msgs)]\n",
    "    # if len(concatted_strings[-1].split(\"\\n\")) % n_msgs:\n",
    "        # TODO: Currently the last incomplete batch is thrown away.\n",
    "        # This can be improved by filling up the batch with the next file batch, but it's quiet tedious\n",
    "        # concatted_strings = concatted_strings[:-1]\n",
    "    return concatted_strings\n",
    "\n",
    "def load_message_df(file: str, cast_dtypes: bool = False, nrows: Optional[int] = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file, dtype=object, header=None, nrows=nrows)\n",
    "    removed_indices = []\n",
    "    if df.iloc[:, -1].isna().sum():\n",
    "        mask = df.iloc[:, -1].isna()\n",
    "        removed_indices = df[~mask].index.to_list()\n",
    "        df = df[mask]\n",
    "    df = df.dropna(axis=1).reset_index(drop=True)\n",
    "    try:\n",
    "        df.columns = MESSAGE_TOKEN_TYPES\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: message file {file} was expected to have {len(MESSAGE_TOKEN_TYPES)} columns\" + \n",
    "                f\" but got {len(df.columns)}). This file will be skipped.\")\n",
    "        return pd.DataFrame()\n",
    "    if cast_dtypes:\n",
    "        df = df.astype(MESSAGE_TOKEN_DTYPE_MAP)\n",
    "    return df, removed_indices\n",
    "\n",
    "def load_orderbook_df(file: str, cast_dtypes: bool = False) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file, dtype=object, header=None)\n",
    "    levels = len(df.columns) // 4\n",
    "    columns = get_orderbook_token_types(levels)\n",
    "    try:\n",
    "        df.columns = columns\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: orderbook file {file} was expected to have {len(columns)} columns,\" + \n",
    "                f\" but got {len(df.columns)}). This file will be skipped.\")\n",
    "        return pd.DataFrame()\n",
    "    if cast_dtypes:\n",
    "        df = df.astype(int)\n",
    "    return df\n",
    "\n",
    "def extract_date(path: Path) -> Optional[str]:\n",
    "    # Use a regex to find the date in the format YYYY-MM-DD\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', path.name)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def convert_to_nanoseconds(s: pd.Series):\n",
    "    split_times = s.str.split('.', expand=True)\n",
    "    split_times.columns = ['seconds', 'fractional']\n",
    "    seconds = split_times['seconds'].astype(int)\n",
    "    fractional = split_times['fractional'].fillna('0').str.ljust(9, '0').str[:9].astype(int)\n",
    "    return seconds * 1_000_000_000 + fractional\n",
    "\n",
    "def merge_dfm_dfo(dfm: pd.DataFrame, dfo: pd.DataFrame, n_msgs: int = 50):\n",
    "    # reduce n_msgs by 1 due to the prepended orderbook row so that in total one batch has again n_msgs elements\n",
    "    n_msgs -= 1\n",
    "    num_blocks = math.ceil(len(dfm) / n_msgs)\n",
    "    merged_len = len(dfm) + num_blocks\n",
    "    merged_df = pd.DataFrame(index=range(merged_len), columns=[*dfm.columns, *dfo.columns])\n",
    "    b_indices = np.arange(num_blocks) * (n_msgs + 1)\n",
    "    b_vals = np.arange(num_blocks) * n_msgs\n",
    "    merged_df.loc[b_indices, dfo.columns] = dfo.iloc[b_vals].values\n",
    "    a_indices = np.setdiff1d(np.arange(merged_len), b_indices)\n",
    "    merged_df.loc[a_indices, dfm.columns] = dfm.values\n",
    "    return merged_df\n",
    "\n",
    "def compute_df_from_file_group(group: list[str], n_msgs: int = 50, only_use_message_orderbook_matches: bool = True, differentiate_time: bool = True):\n",
    "    df, dfm, dfo = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    if only_use_message_orderbook_matches and len(group) != 2:\n",
    "        return pd.DataFrame()\n",
    "    for file in list(group):\n",
    "        file = str(file)\n",
    "        if \"message\" in file:\n",
    "            dfm, removed_indices = load_message_df(file)\n",
    "            if dfm.empty:\n",
    "                continue\n",
    "            if differentiate_time:\n",
    "                dfm[TIME_COL] = convert_to_nanoseconds(dfm[TIME_COL])\n",
    "                dfm[TIME_COL] = dfm[TIME_COL].diff()\n",
    "                dfm = dfm.dropna(subset=TIME_COL).reset_index(drop=True)\n",
    "                dfm[TIME_COL] = dfm[TIME_COL].astype(int)\n",
    "            df = dfm.copy()\n",
    "        elif \"orderbook\" in file:\n",
    "            dfo = load_orderbook_df(file)\n",
    "            if dfo.empty:\n",
    "                continue\n",
    "            df = dfo.copy()\n",
    "        else:\n",
    "            print(f\"File {file} not known. Expected 'orderbook' or 'message' in file\")\n",
    "    if not dfm.empty and not dfo.empty:\n",
    "        if removed_indices:\n",
    "            dfo = dfo.drop(index=removed_indices)\n",
    "        if differentiate_time:\n",
    "            dfo = dfo[1:] # adjust to fit to message dataframe\n",
    "        dfo = dfo.reset_index(drop=True)\n",
    "        df = merge_dfm_dfo(dfm, dfo, n_msgs)\n",
    "    return df\n",
    "    \n",
    "def get_data_stream_generator(data_dir: str, filter_str: str = \"\", n_msgs: int = 50, only_use_message_orderbook_matches: bool = True, \n",
    "                              differentiate_time: bool = True, world_size: int = 1, rank: int = 0, n_files: int = -1) -> Generator[list[str], None, None]:\n",
    "    \"\"\"Generate batches of message files and orderbook files in concatted format (depending on the filter_str) and yield them\n",
    "\n",
    "    :param data_dir: directory of csv files. Expects files to be of format GOOG_2022-01-11_34200000_57600000_message_10.csv\n",
    "    :param batch_size: batch size that is yielded. If None, return single elements, otherwise return \n",
    "        list of length batch size where each element contains n_msgs rows, defaults to None\n",
    "    :param filter_str: optional filter of files, if e.g. only message files should be processed, defaults to \"\"\n",
    "    :param n_msgs: number of messages in one batch element. One message/ row will look like this:\n",
    "\n",
    "        '\n",
    "        \\<time\\>,123345567768,\\<event_type\\>,1,\\<order_id\\>,123456,\\<size\\>,100,\\<price\\>,200,\\<direction\\>,1,\n",
    "        \\<ask_price_1\\>,200,\\<ask_size_1\\>,20,\\<bid_price_1\\>,180,\\<bid_size_1\\>30,\\<ask_price_N\\>,210,\\<ask_size_N\\>,20,\\<bid_price_N\\>,170,\\<bid_size_N\\>30,\n",
    "        '\n",
    "        \n",
    "        defaults to 20\n",
    "    :param only_use_message_orderbook_matches: Whether to only use days where message data and orderbook data is available, defaults to True\n",
    "    :yield: batch or single element of n_msgs\n",
    "    \"\"\"\n",
    "    if filter_str and only_use_message_orderbook_matches:\n",
    "        print(\"Warning: Data files are filtered but 'only_use_message_orderbook_matches' is True,\" +\n",
    "              \" meaning orderbook files and message files are expected. If you want to train purely on one of the two,\" + \n",
    "              \" set 'only_use_message_orderbook_matches' to False\")\n",
    "    filter_str = f\"*{filter_str}*.csv\" if filter_str else \"*.csv\"\n",
    "\n",
    "    # TODO: There is probably a more efficient way to do this, since extract_date is called two times.\n",
    "    # But the array is so small that is basically doesn't matter\n",
    "    msg_files = sorted(list(Path(data_dir).glob(filter_str)), key=extract_date)\n",
    "    file_groups = [(key, list(group)) for key, group in groupby(msg_files, key=extract_date)]\n",
    "    # taken_over_batch = None\n",
    "    # if \"test\" not in data_dir:\n",
    "    #     file_groups = file_groups[26:]\n",
    "    print(f\"Unique days: {len(file_groups)}\")\n",
    "    if n_files > 0 and n_files < len(file_groups):\n",
    "        print(f\"Using first {n_files} of {len(file_groups)}\")\n",
    "        file_groups = file_groups[:n_files]\n",
    "    for _, group in tqdm(file_groups):\n",
    "        print(\"Loading file group \" + str([str(g) for g in group]))\n",
    "        df = compute_df_from_file_group(group, n_msgs, only_use_message_orderbook_matches, differentiate_time)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        dfs = _df_to_str(df, n_msgs)\n",
    "        # if batch_size is None:\n",
    "        if \"test\" in data_dir:  # TODO: FIXME: Remove me possibly\n",
    "            dfs = dfs[:50]  # take only first 50 elements of n_msgs just so validation doesn't take hours\n",
    "        for i, el in enumerate(dfs):\n",
    "            if i % world_size == rank:\n",
    "                yield el\n",
    "        # else:\n",
    "        #     if taken_over_batch:\n",
    "        #         remainder = batch_size - len(batch)\n",
    "        #         batch = taken_over_batch + dfs[:remainder]\n",
    "        #         dfs = dfs[remainder:]\n",
    "        #         taken_over_batch = None\n",
    "        #         yield batch\n",
    "        #     for batch in _batch(dfs, batch_size):\n",
    "        #         if len(batch) == batch_size:\n",
    "        #             yield batch\n",
    "        #         else:\n",
    "        #             taken_over_batch = batch\n",
    "\n",
    "\n",
    "def get_data_stream(data_dir: str, filter_str: str = \"\", n_msgs: int = 50, only_use_message_orderbook_matches: bool = True, \n",
    "                    differentiate_time: bool = True, n_files: int = -1) -> list[str]:\n",
    "    \"\"\"Generate batches of message files and orderbook files in concatted format (depending on the filter_str) and yield them\n",
    "\n",
    "    :param data_dir: directory of csv files. Expects files to be of format GOOG_2022-01-11_34200000_57600000_message_10.csv\n",
    "    :param batch_size: batch size that is yielded. If None, return single elements, otherwise return \n",
    "        list of length batch size where each element contains n_msgs rows, defaults to None\n",
    "    :param filter_str: optional filter of files, if e.g. only message files should be processed, defaults to \"\"\n",
    "    :param n_msgs: number of messages in one batch element. One message/ row will look like this:\n",
    "\n",
    "        '\n",
    "        \\<time\\>,123345567768,\\<event_type\\>,1,\\<order_id\\>,123456,\\<size\\>,100,\\<price\\>,200,\\<direction\\>,1,\n",
    "        \\<ask_price_1\\>,200,\\<ask_size_1\\>,20,\\<bid_price_1\\>,180,\\<bid_size_1\\>30,\\<ask_price_N\\>,210,\\<ask_size_N\\>,20,\\<bid_price_N\\>,170,\\<bid_size_N\\>30,\n",
    "        '\n",
    "        \n",
    "        defaults to 20\n",
    "    :param only_use_message_orderbook_matches: Whether to only use days where message data and orderbook data is available, defaults to True\n",
    "    :yield: batch or single element of n_msgs\n",
    "    \"\"\"\n",
    "    if filter_str and only_use_message_orderbook_matches:\n",
    "        print(\"Warning: Data files are filtered but 'only_use_message_orderbook_matches' is True,\" +\n",
    "              \" meaning orderbook files and message files are expected. If you want to train purely on one of the two,\" + \n",
    "              \" set 'only_use_message_orderbook_matches' to False\")\n",
    "    filter_str = f\"*{filter_str}*.csv\" if filter_str else \"*.csv\"\n",
    "\n",
    "    # TODO: There is probably a more efficient way to do this, since extract_date is called two times.\n",
    "    # But the array is so small that is basically doesn't matter\n",
    "    msg_files = sorted(list(Path(data_dir).glob(filter_str)), key=extract_date)\n",
    "    file_groups = [(key, list(group)) for key, group in groupby(msg_files, key=extract_date)]\n",
    "    dfs = []\n",
    "    print(f\"Unique days: {len(file_groups)}\")\n",
    "    if n_files > 0 and n_files < len(file_groups):\n",
    "        print(f\"Using first {n_files} file_groups out of {len(file_groups)}\")\n",
    "        file_groups = file_groups[:n_files]\n",
    "    for _, group in tqdm(file_groups):\n",
    "        print(\"Loading file group \" + str([str(g) for g in group]))\n",
    "        df = compute_df_from_file_group(group, n_msgs, only_use_message_orderbook_matches, differentiate_time)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        dfs.extend(_df_to_str(df, n_msgs))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95328486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(                 <time> <event_type> <order_id> <size>  <price> <direction>\n",
      "0       34200.017459617            5          0      1  2238200          -1\n",
      "1        34200.18960767            1   11885113     21  2238100           1\n",
      "2        34200.18960767            1    3911376     20  2239600          -1\n",
      "3        34200.18960767            1   11534792    100  2237500           1\n",
      "4        34200.18960767            1    1365373     13  2240000          -1\n",
      "...                 ...          ...        ...    ...      ...         ...\n",
      "269743  57599.872741285            3  286560364    100  2207600          -1\n",
      "269744  57599.903989046            3  287142900    100  2206200          -1\n",
      "269745   57599.95524198            3  286967592    170  2206900          -1\n",
      "269746  57599.958244616            1  287174077    100  2206300          -1\n",
      "269747   57599.95935965            3  287174077    100  2206300          -1\n",
      "\n",
      "[269748 rows x 6 columns], [])\n"
     ]
    }
   ],
   "source": [
    "message_file=load_message_df(\"AMZN_2012-06-21_34200000_57600000_message_10.csv\")\n",
    "print(message_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4aa9c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       <ask_price_1> <ask_size_1> <bid_price_1> <bid_size_1> <ask_price_2>  \\\n",
      "0            2239500          100       2231800          100       2239900   \n",
      "1            2239500          100       2238100           21       2239900   \n",
      "2            2239500          100       2238100           21       2239600   \n",
      "3            2239500          100       2238100           21       2239600   \n",
      "4            2239500          100       2238100           21       2239600   \n",
      "...              ...          ...           ...          ...           ...   \n",
      "269743       2206200          100       2205100          249       2206400   \n",
      "269744       2206400          100       2205100          249       2206500   \n",
      "269745       2206400          100       2205100          249       2206500   \n",
      "269746       2206300          100       2205100          249       2206400   \n",
      "269747       2206400          100       2205100          249       2206500   \n",
      "\n",
      "       <ask_size_2> <bid_price_2> <bid_size_2> <ask_price_3> <ask_size_3>  \\\n",
      "0               100       2230700          200       2240000          220   \n",
      "1               100       2231800          100       2240000          220   \n",
      "2                20       2231800          100       2239900          100   \n",
      "3                20       2237500          100       2239900          100   \n",
      "4                20       2237500          100       2239900          100   \n",
      "...             ...           ...          ...           ...          ...   \n",
      "269743          100       2205000           71       2206500         1290   \n",
      "269744         1290       2205000           71       2206700          170   \n",
      "269745         1290       2205000           71       2206700          170   \n",
      "269746          100       2205000           71       2206500         1290   \n",
      "269747         1290       2205000           71       2206700          170   \n",
      "\n",
      "        ... <bid_price_8> <bid_size_8> <ask_price_9> <ask_size_9>  \\\n",
      "0       ...       2202500         5000       2294300          100   \n",
      "1       ...       2204000          100       2294300          100   \n",
      "2       ...       2204000          100       2267700          100   \n",
      "3       ...       2213000         4000       2267700          100   \n",
      "4       ...       2213000         4000       2267700          100   \n",
      "...     ...           ...          ...           ...          ...   \n",
      "269743  ...       2204300         2300       2207600          100   \n",
      "269744  ...       2204300         2300       2207900         2300   \n",
      "269745  ...       2204300         2300       2208000         3100   \n",
      "269746  ...       2204300         2300       2207900         2300   \n",
      "269747  ...       2204300         2300       2208000         3100   \n",
      "\n",
      "       <bid_price_9> <bid_size_9> <ask_price_10> <ask_size_10> <bid_price_10>  \\\n",
      "0            2202000          100        2298000           100        2189700   \n",
      "1            2202500         5000        2298000           100        2202000   \n",
      "2            2202500         5000        2294300           100        2202000   \n",
      "3            2204000          100        2294300           100        2202500   \n",
      "4            2204000          100        2294300           100        2202500   \n",
      "...              ...          ...            ...           ...            ...   \n",
      "269743       2204200          100        2207900          2300        2204100   \n",
      "269744       2204200          100        2208000          3100        2204100   \n",
      "269745       2204200          100        2208100          1700        2204100   \n",
      "269746       2204200          100        2208000          3100        2204100   \n",
      "269747       2204200          100        2208100          1700        2204100   \n",
      "\n",
      "       <bid_size_10>  \n",
      "0                100  \n",
      "1                100  \n",
      "2                100  \n",
      "3               5000  \n",
      "4               5000  \n",
      "...              ...  \n",
      "269743          3300  \n",
      "269744          3300  \n",
      "269745          3300  \n",
      "269746          3300  \n",
      "269747          3300  \n",
      "\n",
      "[269748 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "orderbook_file=load_orderbook_df(\"AMZN_2012-06-21_34200000_57600000_orderbook_10.csv\")\n",
    "print(orderbook_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX-LOB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
